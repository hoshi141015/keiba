{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "from selenium.webdriver import Chrome,ChromeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    #race_results = pre_race_results\n",
    "    race_results = pre_race_results.copy() #正しくはこちら。注意点で解説。\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            race_results[race_id] = pd.read_html(url)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\t#この部分は動画中に無いですが、捕捉できるエラーは拾った方が、エラーが出たときに分かりやすいです\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "\t        break\n",
    "    return race_results\n",
    "\n",
    "#レース条件のデータ取り出し関数\n",
    "def scrape_race_info(race_id_list):\n",
    "    race_infos={}\n",
    "\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/race/' + race_id\n",
    "            html = requests.get(url)\n",
    "            html.encoding = 'EUC-JP'\n",
    "            soup = BeautifulSoup(html.text,'html.parser')\n",
    "\n",
    "            texts = soup.find(\"div\",attrs = {'class':'data_intro'}).find_all('p')[0].text + \\\n",
    "                soup.find(\"div\",attrs = {'class':'data_intro'}).find_all('p')[1].text\n",
    "            info = re.findall(r\"\\w+\",texts)\n",
    "            info_dict = {}\n",
    "            for text in info:\n",
    "                if text in ['芝','ダート']:\n",
    "                    info_dict['race_type'] = text\n",
    "                if '障' in text:\n",
    "                    info_dict['race_type'] = '障害'\n",
    "                if 'm' in text:\n",
    "                    info_dict['course_len'] = int(re.findall(r'\\d+',text)[0])\n",
    "                if text in ['良','稍重','重','不良']:\n",
    "                    info_dict['ground_state'] = text\n",
    "                if text in ['曇','晴','雨','小雨','小雪','雪']:\n",
    "                    info_dict['weather'] = text\n",
    "                if '年' in text:\n",
    "                    info_dict['date'] = text\n",
    "\n",
    "            race_infos[race_id] = info_dict\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except:\n",
    "            break\n",
    "    return race_infos\n",
    "\n",
    "#pickleデータの読み込み\n",
    "#results = pd.read_pickle('results.pickle')\n",
    "results2 = pd.read_pickle('results_addinfo.pickle')\n",
    "#データの分別\n",
    "def preprocessing(results):\n",
    "    df = results.copy()\n",
    "\n",
    "    # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "    df['着順'] = pd.to_numeric(df['着順'],errors='coerce')\n",
    "    df.dropna(subset=['着順'],inplace=True)\n",
    "    df[\"着順\"] = df[\"着順\"].astype(int)\n",
    "\n",
    "    # 性齢を性と年齢に分ける\n",
    "    df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "    df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "    # 馬体重を体重と体重変化に分ける\n",
    "    df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "    df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1].astype(int)\n",
    "\n",
    "    # データをint, floatに変換\n",
    "    df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "    df[\"course_len\"] = df[\"course_len\"].astype(int)\n",
    "\n",
    "    # 不要な列を削除\n",
    "    df.drop([\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\",'馬名','騎手'], axis=1, inplace=True)\n",
    "\n",
    "    #新しい規格\n",
    "    df['date'] = pd.to_datetime(df['date'],format='%Y年%m月%d日')\n",
    "\n",
    "    return df\n",
    "\n",
    "#ダミーデータの生成\n",
    "def split_data(df,test_size=0.3):\n",
    "    sorted_id_list = df.sort_values('date').index.unique()\n",
    "    train_id_list = sorted_id_list[:round(len(sorted_id_list)*(1-test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list)*(1-test_size)):]\n",
    "    train = df.loc[train_id_list].drop(['date'],axis=1)\n",
    "    test = df.loc[test_id_list].drop(['date'],axis=1)\n",
    "    return train,test\n",
    "\n",
    "class HorseResults:\n",
    "    def __init__(self,horse_results):\n",
    "        self.horse_results = horse_results[['日付','着順','賞金']]\n",
    "        self.preprocessing()\n",
    "        #self.horse_results.rename(columns={'着順':'着順_ave','賞金':'賞金_ave'},inplace=True)\n",
    "\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'],errors='coerce')\n",
    "        df.dropna(subset=['着順'],inplace=True)\n",
    "        #df = df[~(df[\"着順\"].astype(str).str.contains(\"\\D\"))]\n",
    "        df[\"着順\"] = df[\"着順\"].astype(int)\n",
    "\n",
    "        #新しい規格\n",
    "        df['date'] = pd.to_datetime(df['日付'])\n",
    "        df.drop(['日付'],axis=1,inplace=True)\n",
    "\n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0,inplace=True)\n",
    "\n",
    "        self.horse_results = df\n",
    "    def average(self,horse_id_list,date,n_samples='all'):\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "\n",
    "        #過去何年分のデータを取り出すかの設定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date',ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception['n_samples must be >0']\n",
    "        \n",
    "        average = filtered_df.groupby(level = 0)[['着順','賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples),'賞金':'賞金_{}R'.format(n_samples)})\n",
    "\n",
    "    def merge(self,results,date,n_samples = 'all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list,date,n_samples),left_on='horse_id',right_index=True,how='left')\n",
    "        return merged_df\n",
    "    def merge_all(self,results,n_samples = 'all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results,date,n_samples)for date in tqdm(date_list)])\n",
    "        return merged_df\n",
    "\n",
    "class Return:\n",
    "    def __init__(self,return_tables):\n",
    "        self.return_tables = return_tables\n",
    "        #self.fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "        \n",
    "    #変数のように使える()が必要ない\n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "        wins = fukusho[1].str.split('br',expand=True).drop([3],axis=1)\n",
    "        wins.columns = ['win_0','win_1','win_2']\n",
    "        returns = fukusho[2].str.split('br',expand=True).drop([3],axis=1)\n",
    "        returns.columns = ['return_0','return_1','return_2']\n",
    "\n",
    "        df = pd.concat([wins,returns],axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',','')\n",
    "        return df.fillna(0).astype(int)\n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0]=='単勝'][[1,2]]\n",
    "        tansho.columns = ['win','return']\n",
    "\n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column],errors='coerce')\n",
    "        return tansho\n",
    "\n",
    "#_idを使うため不要になった馬名と騎手を切り捨て\n",
    "def prefix(results_fix):\n",
    "    df = results_fix.copy()\n",
    "\n",
    "    df.drop(['馬名','騎手'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self,model,return_tables,std=True):\n",
    "        self.model = model\n",
    "        self.fukusho = Return(return_tables).fukusho\n",
    "        self.tansho = Return(return_tables).tansho\n",
    "        self.std = std\n",
    "    \n",
    "    def predict_proba(self,x):\n",
    "        proba = pd.Series(self.model.predict_proba(x)[:,1], index=x.index)\n",
    "        if self.std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "        return proba\n",
    "\n",
    "    def predict(self,x,threshold=0.5):\n",
    "        y_pred = self.predict_proba(x)\n",
    "        return [0 if p<threshold else 1 for p in y_pred]\n",
    "\n",
    "    def score(self,y_true,x):\n",
    "        return roc_auc_score(y_true,self.predict_proba(x))\n",
    "\n",
    "    def feature_importance(self,x,n_display=20):\n",
    "        importances = pd.DataFrame({'features':x_train.columns,\n",
    "                                    'importance':self.model.feature_importances_})\n",
    "        return importances.sort_values('importance',ascending=False)[:n_display]\n",
    "\n",
    "    def pred_table(self,x,threshold=0.5,bet_only=True):\n",
    "        pred_table = x.copy()[['馬番']]\n",
    "        pred_table['pred'] = self.predict(x,threshold)\n",
    "        if bet_only:\n",
    "            return pred_table[pred_table['pred']==1]['馬番']\n",
    "        else:\n",
    "            return pred_table\n",
    "    \n",
    "    def fukusho_return(self,x,threshold=0.5):\n",
    "        pred_table = self.pred_table(x,threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        df = self.fukusho.copy()\n",
    "        df = df.merge(pred_table,left_index=True, right_index=True,how='right')\n",
    "        for i in range(3):\n",
    "            money += df[df['win_{}'.format(i)]==df['馬番']]['return_{}'.format(i)].sum()\n",
    "        return n_bets,money\n",
    "    def tansho_return(self,x,threshold=0.5):\n",
    "        pred_table = self.pred_table(x,threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        df = self.tansho.copy()\n",
    "        df = df.merge(pred_table,left_index=True, right_index=True,how='right')\n",
    "        money += df[df['win']==df['馬番']]['return'].sum()\n",
    "        return n_bets,money\n",
    "\n",
    "def gain(return_func,x,n_samples=100,lower=50,min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        threshold = 1*i/n_samples + min_threshold * (1-i/n_samples)\n",
    "        n_bets,money = return_func(x,threshold)\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = (n_bets*100 + money)/(n_bets*100)\n",
    "    return pd.Series(gain)\n",
    "\n",
    "def process_categorical(df,target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    #target_columns以外にカテゴリ変数があれば、ダミー変数にする\n",
    "    df2 = pd.get_dummies(df2)\n",
    "    \n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = []\n",
    "for place in range(1, 11, 1):\n",
    "    for kai in range(1, 6, 1):\n",
    "        for day in range(1, 13, 1):\n",
    "            for r in range(1, 13, 1):\n",
    "                race_id = \"2020\" + str(place).zfill(2) + str(kai).zfill(2) +\\\n",
    "\t\t        str(day).zfill(2) + str(r).zfill(2)\n",
    "                race_id_list.append(race_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1b7f635091433fb155a72cfbb90cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#スクレイピング\n",
    "race_infos = scrape_race_info(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
